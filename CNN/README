Image_Classification_part_1
------------------------------------------------------------
In this tutorial, we will build a Cat vs. Dog Image Classification using a Convnets from Scratch

NOTE: The 2,000 images used in this exercise are excerpted from the "Dogs vs. Cats" dataset available on Kaggle, 
which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.

#################################################################################################


Image_Classification_part_2
------------------------------------------------------------
In this notebook we will build on the model we created in previous one to classify cats vs. dogs, and 
improve accuracy by employing a couple strategies to reduce overfitting: data augmentation and dropout.

We will follow these steps:

-Explore how data augmentation works by making random transformations to training images.
-Add data augmentation to our data preprocessing.
-Add dropout to the convnet.
-Retrain the model and evaluate loss and accuracy.
#################################################################################################


Image_Classification_part_3
------------------------------------------------------------
In part 1, we built a convnet from scratch, and were able to achieve an accuracy of about 70%. 
With the addition of data augmentation and dropout in part 2, we were able to increase accuracy to about 80%. 
That seems decent, but 20% is still too high of an error rate. Maybe we just don't have enough training data available 
to properly solve the problem. What other approaches can we try?

In this notebook, we'll look at two techniques for repurposing feature data generated from image models that have 
already been trained on large sets of data, feature extraction and fine tuning, and use them to improve the 
accuracy of our cat vs. dog classification model.
